# Hutter prize

Link to the prize: [http://prize.hutter1.net](http://prize.hutter1.net).

Link to the Large Text Compression Benchmark (LCB)
[http://mattmahoney.net/dc/text.html](http://mattmahoney.net/dc/text.html).


## Resources:
- [Arithmetic conding](https://en.wikipedia.org/wiki/Arithmetic_coding)
	* Maybe allows to turn lossy compression into lossless. Hypothesis: use lossy
	model to create pob dist and use AE to enconde.
	* Some guys implementation here: [https://www.nayuki.io/page/reference-arithmetic-coding](https://www.nayuki.io/page/reference-arithmetic-coding)

- [Context mixing](https://en.wikipedia.org/wiki/Context_mixing). Other classic lossless data compression tecnique.

- [Nueral Compression](https://robamler.github.io/teaching/compress21/). Lectures of some university in germany.

- [Transformer-XL](https://arxiv.org/pdf/1901.02860.pdf)
	* Top guy on LCB use it.
	* DL implies transformers nowadays so we may as well use it for its main
	  purpose (text)

- [nncp](https://bellard.org/nncp)
	* Top one in LCB. Web and two papers:
		- [https://bellard.org/nncp/nncp.pdf](https://bellard.org/nncp/nncp.pdf)
		- [https://bellard.org/nncp/nncp_v2.1.pdf](https://bellard.org/nncp/nncp_v2.1.pdf)


## Motivations
- [AIXI](http://www.hutter1.net/ai/uaibook.htm) => Compression == Intelligence
- Guy was on [Lex fridman](https://www.youtube.com/watch?v=E1AxVXt2Gv4) so must be important.
